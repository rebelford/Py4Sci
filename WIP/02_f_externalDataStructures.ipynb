{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b7628f-f75a-4756-a7f0-6a37b20c65a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Non-Volatile External Data Structures\n",
    "Python’s built-in structures (lists, tuples, dictionaries, etc.) exist in **volatile memory** (RAM) and disappear when the program ends. External data structures are stored in **non-volatile memory** and persist beyond program execution. These include:\n",
    "\n",
    "1. **File-Based Data Structures**\n",
    "   - **Text-Based Files (Structured/Unstructured)**\n",
    "     - `.txt`: Raw text files (unstructured).\n",
    "     - `.csv`: Tabular data (structured, human-readable, but limited).\n",
    "     - `.json`: Key-value format (structured, hierarchical, readable).\n",
    "   - **Binary Files**\n",
    "     - `.pickle`: Stores Python objects in a serialized format (not human-readable).\n",
    "     - `.npy/.npz`: NumPy’s binary storage for efficient numerical data.\n",
    "\n",
    "2. **Databases (Structured, Queryable External Data)**\n",
    "   - **SQL Databases (Relational)**\n",
    "     - Store data in structured tables with defined relationships.\n",
    "     - Examples: SQLite, PostgreSQL, MySQL.\n",
    "   - **NoSQL Databases (Flexible, Key-Value, Document-Based)**\n",
    "     - Store unstructured or semi-structured data in key-value or document formats.\n",
    "     - Examples: MongoDB (documents), Redis (key-value pairs).\n",
    "\n",
    "3. **Web APIs and Networks as External Data Sources**\n",
    "   - Accessing data from remote servers (e.g., PubChem, weather services).\n",
    "   - Often return data in JSON, XML, or other standardized formats.\n",
    "   - Unlike local files or databases, APIs require a network connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea59b42d-a58b-417f-ae52-9293e2430f8d",
   "metadata": {},
   "source": [
    "## Understanding Input/Output (I/O)\n",
    "\n",
    "At its core, **Input/Output (I/O)** refers to any communication between a program and the outside world. It is not limited to data storage and retrieval; it also includes interactions with users, hardware, and network resources. I/O operations can be broadly categorized into:\n",
    "\n",
    "1. **User Interaction**  \n",
    "   - Input: Receiving user input via `input()` or GUI elements.  \n",
    "   - Output: Displaying text via `print()`, rendering graphics, or updating a UI.\n",
    "\n",
    "2. **File I/O** (Non-volatile Storage)  \n",
    "   - Reading and writing data to files (e.g., `.txt`, `.csv`, `.json`, `.pickle`).\n",
    "   - Persistent storage that remains available after the program terminates.\n",
    "\n",
    "3. **Network I/O**  \n",
    "   - Communicating with remote servers, APIs, or databases over the internet.\n",
    "   - Sending and receiving data over sockets (e.g., accessing PubChem via an API).\n",
    "\n",
    "4. **Inter-process and Hardware I/O**  \n",
    "   - Communicating with external devices like sensors, databases, or microcontrollers.\n",
    "   - Data exchange between different programs or services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b6a641-8ed4-43ba-9b9d-f6ce0b703555",
   "metadata": {},
   "source": [
    "## Understanding Interfaces\n",
    "Inherent in IO operations is the interface between two entities or systems and we are going to need to introduce the concept of an API (Application Program Interface).  If you look at the above IO systems you realize there are human, hardware and sofware components, and so there are different types of interfaces. The following table gives an overview of several interfaces, and as a human, you have used both CLIs and GUIs in this class.\n",
    "\n",
    "| **Interface Type** | **Example** | **Who/What Interacts?** |\n",
    "|-------------------|------------|-------------------|\n",
    "| **Graphical User Interface (GUI)** | Windows, Web Apps | **User ↔ System** (via visual elements like buttons, menus) |\n",
    "| **Command Line Interface (CLI)** | Terminal, Bash, Python REPL | **User ↔ System** (via text commands) |\n",
    "| **Application Programming Interface (API)** | REST API, Database API | **Software ↔ Software** (via structured requests & responses) |\n",
    "| **Hardware Interfaces** | USB, HDMI, Bluetooth | **Physical Devices ↔ System** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc77dfd8-598a-472a-b78b-8498681b2d27",
   "metadata": {},
   "source": [
    "# File Based Data Structures\n",
    "Before we proceed, we are going to install two new third party packages; Seaborn and pandas.  Seaborn is a visualization package built on Matplotlib and it comes with a series of files we can use for various data explorations.  Pandas is a data manipulation package built on Numpy and is widely used to handle structured data like csv, json, SQL, Excel...)\n",
    "\n",
    "Open your Ubuntu terminal and activate your virtual environment (mine is py4sci)\n",
    "```bash \n",
    "conda activate py4sci\n",
    "``` \n",
    "The (base) in front of the command prompt should change to (py4sci). Now that you have activated your virtual environment you are ready to install the packages in it, and we will do them one at a time, although you could do them both at once.\n",
    "```bash\n",
    "conda install -c conda-forge seaborn\n",
    "conda install -c conda-forge pandas\n",
    "```\n",
    "Let's now look at the data sets we downloaded when we installed Seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a593a112-bbab-4258-b886-6c0bf4d5e7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "halogrens = ['F', 'Cl', 'Br', 'I', 'At', 'Ts'] \n",
      "atomic number = [9, 17, 35, 53, 85, 117] \n",
      "atomic_masses = [18.998, 35.45, 79.904, 126.9, 210, 294] \n",
      "electronegativities = [3.98, 3.16, 2.96, 2.66, 2.2, None]\n"
     ]
    }
   ],
   "source": [
    "# List of halogen symbols\n",
    "halogens = [\"F\", \"Cl\", \"Br\", \"I\", \"At\", \"Ts\"]\n",
    "\n",
    "# Atomic numbers\n",
    "atomic_numbers = [9, 17, 35, 53, 85, 117]\n",
    "\n",
    "# Atomic masses (g/mol)\n",
    "atomic_masses = [18.998, 35.45, 79.904, 126.90, 210, 294]\n",
    "\n",
    "# Electronegativity (Pauling scale)\n",
    "electronegativities = [3.98, 3.16, 2.96, 2.66, 2.2, None]  # Ts unknown\n",
    "\n",
    "# Boiling points (K)\n",
    "boiling_points = [85.03, 239.11, 332.0, 457.4, 610, None]  # Ts unknown\n",
    "print(f\"halogrens = {halogens} \\natomic number = {atomic_numbers} \\natomic_masses = {atomic_masses} \\\n",
    "\\nelectronegativities = {electronegativities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c545116d-04f4-4f17-b9dc-e3715fb31c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c9a9b-efe2-44ab-a310-f3cae8df01bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76efe002-c571-4d7f-9de9-88939623cab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a8170-2b0a-424d-bf2e-02b791d1bd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33596d9c-ae44-44db-b3e5-97768cad6e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140bbde7-c599-4a06-9739-5d48dcbf5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory structure\n",
    "base_data_dir = os.path.expanduser(\"~/data\")  # Parent directory\n",
    "pubchem_data_dir = os.path.join(base_data_dir, \"pubchem_data\")  # Subdirectory for PubChem\n",
    "os.makedirs(pubchem_data_dir, exist_ok=True)  # Ensure directories exist\n",
    "\n",
    "# Define file URL and local path\n",
    "file_url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/periodictable/CSV?response_type=save&response_basename=PubChemElements_all\"\n",
    "local_file_path = os.path.join(pubchem_data_dir, \"PubChemElements_all.csv\")\n",
    "\n",
    "# Download the file\n",
    "print(f\"Downloading PubChem CSV to: {local_file_path} ...\")\n",
    "urllib.request.urlretrieve(file_url, local_file_path)\n",
    "print(\"Download complete!\")\n",
    "\n",
    "# Verify if the file was saved\n",
    "if os.path.exists(local_file_path):\n",
    "    print(f\"File successfully saved at: {local_file_path}\")\n",
    "\n",
    "    # Load into Pandas DataFrame\n",
    "    df = pd.read_csv(local_file_path)\n",
    "    print(\"\\nFirst few rows of the dataset:\")\n",
    "    print(df.head())  # Display first few rows\n",
    "\n",
    "else:\n",
    "    print(\"Download failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230ec40-0415-48a8-841b-9a8923090f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.expanduser(\"~/data/pubchem_data/PubChemElements_all.csv\")\n",
    "\n",
    "# Open the CSV file and read the data\n",
    "with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)  # Read the CSV file\n",
    "    rows = list(reader)  # Convert to a list of lists\n",
    "\n",
    "# Extract headers (column names)\n",
    "headers = rows[0]  # First row contains the column names\n",
    "\n",
    "# Extract all column data as lists (skip the header row)\n",
    "columns = {header: [] for header in headers}  # Dictionary to hold columns as lists\n",
    "\n",
    "for row in rows[1:]:  # Skip header row\n",
    "    for col_index, value in enumerate(row):\n",
    "        columns[headers[col_index]].append(value)\n",
    "\n",
    "# Construct dictionary of elements\n",
    "elements_dict = {}\n",
    "\n",
    "for i in range(len(columns[\"Symbol\"])):  # Iterate over element rows\n",
    "    element_symbol = columns[\"Symbol\"][i]  # Get element symbol\n",
    "    element_data = {headers[j]: columns[headers[j]][i] for j in range(len(headers))}  # Create embedded dictionary\n",
    "\n",
    "    elements_dict[element_symbol] = element_data  # Assign to main dictionary\n",
    "\n",
    "# Display a sample of the final dictionary\n",
    "sample_element = \"I\"  # Example: Chlorine\n",
    "print(f\"Data for {sample_element}:\")\n",
    "print(elements_dict.get(sample_element, \"Element not found!\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83ae54-2aec-44b6-82e6-dd64096afb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the JSON file path\n",
    "json_file_path = os.path.expanduser(\"~/data/pubchem_data/elements_data.json\")\n",
    "\n",
    "# Save the dictionary as a JSON file\n",
    "with open(json_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(elements_dict, json_file, indent=4)\n",
    "\n",
    "print(f\"Full periodic table data saved to: {json_file_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Reload JSON later and display sample\n",
    "# -------------------------------\n",
    "print(\"\\nReloading data from JSON...\")\n",
    "\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    loaded_elements = json.load(json_file)  # Load back into a dictionary\n",
    "\n",
    "# Display data for Oxygen as an example\n",
    "sample_element = \"O\"\n",
    "print(f\"\\nData for {sample_element} (from JSON):\")\n",
    "print(loaded_elements.get(sample_element, \"Element not found!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2612540d-dcea-46f8-9acb-22fd532b565a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'Ar', 'K', 'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'Ga', 'Ge', 'As', 'Se', 'Br', 'Kr', 'Rb', 'Sr', 'Y', 'Zr', 'Nb', 'Mo', 'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'I', 'Xe', 'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd', 'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi', 'Po', 'At', 'Rn', 'Fr', 'Ra', 'Ac', 'Th', 'Pa', 'U', 'Np', 'Pu', 'Am', 'Cm', 'Bk', 'Cf', 'Es', 'Fm', 'Md', 'No', 'Lr', 'Rf', 'Db', 'Sg', 'Bh', 'Hs', 'Mt', 'Ds', 'Rg', 'Cn', 'Nh', 'Fl', 'Mc', 'Lv', 'Ts', 'Og'])\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "json_file_path = \"~/data/pubchem_data/elements_data.json\"\n",
    "\n",
    "with open(os.path.expanduser(json_file_path), \"r\", encoding=\"utf-8\") as json_file:\n",
    "    elements_data = json.load(json_file)\n",
    "\n",
    "# Print all stored halogens\n",
    "print(elements_data.keys())  # Output: dict_keys(['F', 'Cl', 'Br', 'I', 'At', 'Ts'])\n",
    "print(type(elements_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4be849-a28a-4cf3-b379-69b81e7d3639",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03fbc648-5cde-4c42-9f8b-208f5701fdbc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "484280f0-ae91-4719-9f50-3779d6ff0368",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "| **Type**       | **Source Example** | **Data Format** | **Storage/Query Method** |\n",
    "|--------------|-----------------|--------------|-----------------|\n",
    "| **File-Based Data** | JSON, CSV, Pickle, HDF5 | Structured | File I/O (pandas, NumPy) |\n",
    "| **Relational Databases** | PubChem SQL, NOAA Climate Database | SQL (Structured) | Queries (SQL, Relational Model) |\n",
    "| **NoSQL Databases** | MongoDB, Firebase | JSON, BSON (Semi-Structured) | Queries (NoSQL, Document Store) |\n",
    "| **APIs (REST)** | PubChem API, OpenWeather API, NASA API | JSON, XML, CSV | RESTful Requests (`GET`, `POST`) |\n",
    "| **SPARQL APIs (Linked Data)** | Wikidata, PubChem RDF, DBpedia, Gene Ontology | RDF/XML, Turtle, SPARQL-JSON | SPARQL Queries (Graph-Based) |\n",
    "| **Graph Databases (RDF Stores)** | Blazegraph, Virtuoso, Neo4j (for Linked Data) | RDF, GraphML | Queries (SPARQL, Cypher) |\n",
    "| **Web Scraping** | Wikipedia, Research Articles, Google Scholar | HTML (Semi-Structured) | Parsing (BeautifulSoup, Scrapy) |\n",
    "| **Streaming Data** | Twitter API, IoT Sensor Networks | JSON, Avro, Protobuf | WebSockets, Kafka, MQTT |\n",
    "\n",
    "## How Databases and Web Services Fit Into the Picture\n",
    "They are an interesting hybrid, they store non-volatile data from the perspective of the python program, but access to them introduces elements of volatility:\n",
    "1. **Non-Volatile:** Data persists beyond program execution, just like files.\n",
    "2. **Volatile:** Unlike files, a database can be unavailable (e.g., server downtime), and data integrity can be affected by concurrent access.\n",
    "\n",
    "In a structured **progression from volatile to non-volatile** data, databases fit in as **external but queryable structures**:\n",
    "- Unlike files, they support **efficient querying** (e.g., SQL queries).\n",
    "- Unlike in-memory Python data structures, they **persist beyond execution**.\n",
    "- Unlike JSON or CSV files, they allow **dynamic updates and complex relationships**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118d329-7f1c-43d3-bace-718ded251cf4",
   "metadata": {},
   "source": [
    "### Brief Overview of APIs\n",
    "We will use Application Program Interfaces (APIs) throughout this course as they enable software programs to communicate with each other. The API defines a set of rules that allow the two programs to communicate with each other, and when that communication involves external data structures the API defines:\n",
    "\n",
    " **1. Endpoint (Exposes Data)**\n",
    "   - The URL where the API listens.\n",
    "   - Example: `https://query.wikidata.org/sparql`\n",
    "\n",
    " **2. Request/Response Model (How Data is Sent and Received)**\n",
    "   - Defines the protocol and request method.\n",
    "   - **REST APIs**: `GET`, `POST` over HTTP.\n",
    "   - **SPARQL APIs**: SPARQL Query over HTTP (`GET`/`POST`).\n",
    "   - **Database APIs**: SQL queries over TCP/IP.\n",
    "\n",
    " **3. Data Format (How Data is Structured)**\n",
    "   - **REST APIs**: JSON, XML, CSV.\n",
    "   - **SPARQL APIs**: SPARQL-JSON, RDF/XML, Turtle, N-Triples.\n",
    "   - **Databases**: Tabular data (Relational Tables).\n",
    "   - **GraphQL APIs**: JSON (Custom queries).\n",
    "     \n",
    "We will dive deeper into APIs as the course proceeds, but you need to understand what they are and what they do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905dc70-49cf-4eac-8fa6-034a6e8f73e1",
   "metadata": {},
   "source": [
    "# What is a Database? \n",
    "\n",
    "A **database** is like a super-organized digital filing system where you store, manage, and retrieve information efficiently. Instead of using multiple text files or spreadsheets, a database helps keep data structured, searchable, and scalable.  \n",
    "\n",
    "Imagine you’re running a **chemical inventory** for a lab. You could store data in a spreadsheet (`CSV file`), but as the dataset grows, searching, updating, and ensuring accuracy become difficult. A **database** solves this by organizing data into a structured system that allows for efficient searching, sorting, and updating.\n",
    "\n",
    "---\n",
    "\n",
    "## SQL vs. NoSQL: Two Ways to Organize a Database\n",
    "Databases come in two major types: **SQL (Structured Query Language) databases** and **NoSQL (Not Only SQL) databases**.  \n",
    "\n",
    "### SQL Databases: Like a Well-Organized Filing Cabinet\n",
    "📂 **Think of SQL databases as an Excel spreadsheet where everything is stored in structured tables.**  \n",
    "- Data is organized into **tables** with **rows (records)** and **columns (fields)**.  \n",
    "- You must **predefine the structure** (e.g., a table for chemical compounds must always have columns like “Name,” “Formula,” and “Molecular Weight”).  \n",
    "- Data is retrieved using **SQL queries**, like asking a librarian for a specific book.  \n",
    "- Best for structured data with **relationships** (e.g., linking a chemical sample to a supplier).  \n",
    "- Examples: **SQLite, PostgreSQL, MySQL, Microsoft SQL Server.**  \n",
    "\n",
    "📌 **Analogy:**  \n",
    "- If you store lab results in a filing cabinet with labeled folders, SQL databases ensure everything follows a strict structure.\n",
    "- Example SQL query:  \n",
    "  ```sql\n",
    "  SELECT * FROM chemicals WHERE molecular_weight > 200;\n",
    "  ```\n",
    "  *(Find all chemicals with a molecular weight above 200.)*\n",
    "\n",
    "---\n",
    "\n",
    "### NoSQL Databases: Like a Digital Whiteboard\n",
    "📝 **NoSQL databases are more flexible, like sticky notes on a whiteboard that can change anytime.**  \n",
    "- Instead of structured tables, data is stored as **documents, key-value pairs, graphs, or columns**.  \n",
    "- You don’t need a strict structure—different records can have different fields.  \n",
    "- Great for **large, flexible datasets** that may change often (e.g., tracking real-time sensor data).  \n",
    "- Common in big data, web applications, and IoT (Internet of Things).  \n",
    "- Examples: **MongoDB (document-based), Redis (key-value), Cassandra (column-based).**  \n",
    "\n",
    "📌 **Analogy:**  \n",
    "- If SQL is like an Excel spreadsheet, **NoSQL is like a collection of Google Docs**—each document can have a different format.  \n",
    "- Example MongoDB NoSQL document (JSON-like format):  \n",
    "  ```json\n",
    "  {\n",
    "    \"name\": \"Acetone\",\n",
    "    \"formula\": \"C3H6O\",\n",
    "    \"properties\": {\n",
    "      \"molecular_weight\": 58.08,\n",
    "      \"boiling_point\": 56.5\n",
    "    }\n",
    "  }\n",
    "  ```\n",
    "  *(Flexible—some documents may have extra fields, some may not.)*\n",
    "\n",
    "---\n",
    "\n",
    "#### How to Choose?\n",
    "| Feature         | SQL Databases (Structured) | NoSQL Databases (Flexible) |\n",
    "|---------------|-------------------------|-------------------------|\n",
    "| **Data Structure** | Organized in tables (rows & columns) | Documents, key-value, graphs |\n",
    "| **Flexibility**  | Fixed structure (schema) | Dynamic structure (schema-less) |\n",
    "| **Best For** | Structured, relational data (e.g., patient records, inventory) | Big data, fast-changing data (e.g., IoT, social media) |\n",
    "| **Query Language** | Uses SQL (structured queries) | Uses APIs, JSON-like queries |\n",
    "| **Examples** | SQLite, PostgreSQL, MySQL | MongoDB, Redis, Firebase |\n",
    "\n",
    "---\n",
    "\n",
    "#### Putting It All Together\n",
    "- **SQL is best for structured, well-defined data** (like lab inventory).  \n",
    "- **NoSQL is better for rapidly changing, unstructured data** (like real-time sensor readings).  \n",
    "- Both are **external data storage** solutions that move data from volatile (RAM) to non-volatile storage, ensuring persistence.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4892769f-273d-447d-bcfe-01bb7b59c08a",
   "metadata": {},
   "source": [
    "# Semantic Web: A Scientific Data Perspective\n",
    "The **Semantic Web** is an extension of the traditional World Wide Web that enables machines to understand and process data in a structured and meaningful way. It transforms the web from a collection of documents intended for human reading into a globally linked network of **structured data** that software can interpret, query, and reason over.\n",
    "\n",
    "At the heart of the Semantic Web are **linked data technologies**, which define **data relationships** instead of just formatting data into tables or files. The Semantic Web allows Python scripts to query machine-readable datasets. This means scientists can directly access structured data on molecules, atmospheric data, physical constants, and more. The key components include:\n",
    "\n",
    "1. **RDF (Resource Description Framework)**\n",
    "   - The **data model** of the Semantic Web.\n",
    "   - Represents **facts as triples**: **subject → predicate → object**.\n",
    "   - Example (Chemical Properties in RDF):\n",
    "     ```\n",
    "     <Acetone>   <has_molecular_weight>   \"58.08 g/mol\"\n",
    "     ```\n",
    "   - Enables **machine-readable relationships**, making **chemical knowledge searchable**.\n",
    "\n",
    "2. **OWL (Web Ontology Language)**\n",
    "   - Extends RDF with **logic & reasoning**.\n",
    "   - Defines **ontologies**: hierarchical classifications of concepts.\n",
    "   - Example: If **Acetone is a Ketone**, and **Ketones are Organic Compounds**, OWL **infers** that **Acetone is an Organic Compound**.\n",
    "\n",
    "3. **SPARQL (Query Language for RDF)**\n",
    "   - The SQL of the Semantic Web.\n",
    "   - Allows **querying linked datasets** across the web.\n",
    "\n",
    "4. **Linked Open Data (LOD)**\n",
    "   - RDF-based datasets are interlinked across disciplines.\n",
    "   - **Example Datasets**:\n",
    "     - **Wikidata** (General knowledge)\n",
    "     - **PubChem RDF** (Chemical data)\n",
    "     - **DBpedia** (Structured Wikipedia data)\n",
    "     - **Gene Ontology** (Biological data)\n",
    "\n",
    "## **Semantic Web vs Traditional Databases**\n",
    "| **Feature**          | **Relational Databases (SQL, NoSQL)** | **Semantic Web (RDF, OWL, SPARQL)** |\n",
    "|--------------------|--------------------------------|--------------------------------|\n",
    "| **Structure**      | Tables (structured schema)    | Graph-based (triples: subject-predicate-object) |\n",
    "| **Queries**       | SQL                            | SPARQL |\n",
    "| **Flexibility**   | Rigid schema                  | Dynamic relationships (easily extendable) |\n",
    "| **Interoperability** | Limited to specific database engines | Global linking of datasets (LOD cloud) |\n",
    "| **Examples**      | PostgreSQL, MongoDB           | Wikidata, PubChem RDF, DBpedia |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843e2ea7-6d8d-476c-b9e3-2000c91a8366",
   "metadata": {},
   "source": [
    "#  Web Scraping and Databases: A Hybrid Approach\n",
    "While web scraping is often used to extract data for immediate use, it does not store it. A powerful workflow would be:\n",
    "- **Scrape data** from online sources.\n",
    "- **Store it in a structured database (SQL or NoSQL)** for long-term analysis.\n",
    "- **Query it later** instead of repeatedly scraping.\n",
    "\n",
    "\n",
    "## Web Scraping as a Data Acquisition Method\n",
    "Web scraping is a method of extracting **external data** from structured or semi-structured sources on the web and transforming it into a usable format. Unlike databases or file storage, web scraping **does not inherently store data**—it is a way to retrieve and structure data from the web dynamically. It allows access to **data stored in HTML web pages** that might not be available via an API.\n",
    "\n",
    "### Web Scraping vs. APIs\n",
    "| Feature         | Web Scraping | APIs |\n",
    "|---------------|-------------|------|\n",
    "| **Access** | Extracts data from web pages (HTML tables, text, lists) | Queries structured data from a service (often JSON or XML) |\n",
    "| **Structure** | Often semi-structured (needs parsing) | Well-structured |\n",
    "| **Reliability** | Pages may change, breaking the scraper | More stable (unless API changes) |\n",
    "| **Use Case** | Extracting tables, research data, metadata from articles | Accessing structured datasets (PubChem, NCBI, weather data) |\n",
    "\n",
    "Thus, **web scraping is an alternative to APIs when structured access is unavailable**.\n",
    "\n",
    "---\n",
    "\n",
    "## Web Scraping as a Bridge from Classical Literature to Structured Data\n",
    "Scientific data has historically been communicated through **journal articles, textbooks, and reports**. Many modern scientific knowledge repositories (e.g., Wikipedia, research databases) still store information in text-based formats rather than structured databases. Web scraping allows you to:\n",
    "\n",
    "- Extract **tabular data** (like chemical properties from Wikipedia or patents).\n",
    "- Retrieve **text-based metadata** (such as author names, abstracts, and citations).\n",
    "- Collect **non-tabular structured information** (like structured web pages with lists of elements).\n",
    "\n",
    "By applying **text parsing, table extraction, and structured storage**, web scraping allows researchers to **convert human-readable content into machine-readable data**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f663f1-964f-448c-bff7-a981684e41d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9803b6d-4870-426b-8e02-b72418ef79e0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(py4sci)",
   "language": "python",
   "name": "py4sci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
